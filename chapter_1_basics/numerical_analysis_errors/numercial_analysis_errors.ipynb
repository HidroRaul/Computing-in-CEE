{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to numerical error analysis\n",
    "\n",
    "by Xiaofeng Liu, Ph.D., P.E.\n",
    "Associate Professor\n",
    "\n",
    "Department of Civil and Environmental Engineering\n",
    "\n",
    "Institute of Computational and Data Sciences\n",
    "\n",
    "Penn State University\n",
    "\n",
    "223B Sackett Building, University Park, PA 16802\n",
    "\n",
    "Web: http://water.engr.psu.edu/liu/\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "## Classification of errors:\n",
    "* Roundoff error: caused by finite number of bits used in computer to represent a value. For example, the use of 32 bits or 64 bits to present a float number is called single precision and double precision, respectively. They can only have 7 or 13 significant decimals, respectively. This can cause problem, for example, for the calculation of the difference between two close values.\n",
    "\n",
    "* Approximation error: for example, in differential equations, exact derivative is approximated by algebraic difference scheme:\n",
    "\\begin{equation}\n",
    "slope: \\frac{dy}{dx} \\approx \\frac{y_2 - y_1}{x_2 - x_1}\n",
    "\\end{equation}\n",
    "\n",
    "* Iteration error: error in iterative methods. For an iterative method to converge, as iteration number increases, the difference between calculated result and exact solution should go to zero. However, we can only perform finite number of iterations, which induces iteration error.\n",
    "\n",
    "## Measure of errors\n",
    "\n",
    "To measure the magnitude of error, we define absolute error and relative error. However, in order to know the error, we need to know the true value. In reality, the true value may be unknown before hand, for example, the roots of a very complex high-order polynomial. The best we have on hand may be just some estimations of the true value. Thus, we need to consider the following two scenarios. \n",
    "\n",
    "### When the true value is known\n",
    "If we know the true value and we have an estimation, then the absolute error is defined as \n",
    "\\begin{equation}\n",
    "e_t = True \\, value - approximation\n",
    "\\end{equation}\n",
    "and the relative error is defined as\n",
    "\\begin{equation}\n",
    "\\epsilon_t = \\frac{True \\, value - approximation}{True \\, value} \\times 100\\%\n",
    "\\end{equation}\n",
    "In the above, the subscript $t$ signifies the use of \"true\" value. It is noted that in the above definition of relative errors, it is assumed the true value is not zero. Otherwise, there will be a \"division by zero\" error. The following is an example for such calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolute error =  0.021592599999999962\n",
      "relative error =  0.6873138165655203 %\n"
     ]
    }
   ],
   "source": [
    "True_value = 3.1415926\n",
    "approximation = 3.12\n",
    "\n",
    "e_t = True_value - approximation\n",
    "epsilon_t = (True_value - approximation)/True_value*100\n",
    "\n",
    "print(\"absolute error = \", e_t)\n",
    "print(\"relative error = \", epsilon_t, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When the true value is unknow\n",
    "This usually happens when we use some iterative scheme to seek a solution. The hope is to improve our estiamtion after each additional iteration. Let $\\widetilde{x_i}$ and $\\widetilde{x_{i+1}}$ be the approximation at iteration $i$ and $i+1$, respectively. Then the absolute error and relative error can be defined as\n",
    "\\begin{equation}\n",
    "e_a = \\widetilde{x_{i+1}} - \\widetilde{x_{i}}\n",
    "\\end{equation}\n",
    "and the relative error is defined as\n",
    "\\begin{equation}\n",
    "\\epsilon_a = \\frac{\\widetilde{x_{i+1}}-\\widetilde{x_{i}}}{\\widetilde{x_{i+1}}} \\times 100\\%\n",
    "\\end{equation}\n",
    "In the above, the subscript $a$ signifies the use of \"approxmation\". In most of computing methods using iterative schemes, these definitions of errors are used as convergence criteria for stopping the iterations. \n",
    "\n",
    "The following is an example. Assuming we have an iterative scheme which generates a sequence of (increasingly better) approximations. In this sequence, we have two successive approximations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolute error =  0.020000000000000018\n",
      "relative error =  0.6369426751592362 %\n"
     ]
    }
   ],
   "source": [
    "x_10 = 3.12\n",
    "x_11 = 3.14\n",
    "\n",
    "e_a = x_11-x_10\n",
    "epsilon_a =(x_11-x_10)/x_11*100\n",
    "\n",
    "print(\"absolute error = \", e_a)\n",
    "print(\"relative error = \", epsilon_a, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number representation in number systems:\n",
    "The \"significant digits\" is the measure of accuracy of a number (resolution if the data is from measurement). It included all digits excluding all leading zeros. For example the number 0.0045 has two significant digits, and 123.00 has five significant digits. Note that for a float, all zeros to the right of last non-zero digit should be counted as significant. For example, the number 0.004500 has four significant digits.\n",
    "\n",
    "Numbers can be represented using different number systems:\n",
    "* The base-10 system (decimal): common for human communication (maybe rooted in the fact that every human has ten fingers). For example, $346=3 \\times 10^2+4\\times10^1+6\\times10^0$\n",
    "* The base-2 system (binary): this is used in digital computers. A number is represented by a number of binary bits. For example, for a eight-bit unsigned integer number\n",
    "\\begin{equation}\n",
    "10010101 = 1\\times 2^7 + 0\\times 2^6 + 0 \\times 2^5+ 1 \\times 2^4+ 0 \\times 2^3+ 1 \\times 2^2+ 0 \\times 2^1+ 1 \\times 2^0 = 149\n",
    "\\end{equation}\n",
    "\n",
    "Python provides an easy way to convert between binary and decimal. An example of converting a binary string to decimal is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int('10010101',2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or simply add \"0b\" in front of the binary string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0b10010101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, an example of converting a decimal to a binary is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0b10010101'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin(149)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-0b10010101'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin(-149)   #add a negative sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above methods only work for unsigned and singed integers. For float, the conversion is more complicated. There is no standard Python function for this conversion. However, there are other ways. For example, one can write a conversion function or use packages such as \"bitstring\". The package \"bitstring\" is not installed with Anaconda by default. So if you want to use it, you need to first install it. You can use the following command (may not work on a shared computer that you do not have the privilege to install new packages):\n",
    "```\n",
    "conda install -c bioconda bitstring\n",
    "```\n",
    "or\n",
    "```\n",
    "pip install bitstring\n",
    "```\n",
    "\n",
    "Then you can use the following code for conversion. See the documentation of \"bitstring\" for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000000101001101100101100101001\n",
      "0100011010000101010011001001101101001000100111110111011011100101\n"
     ]
    }
   ],
   "source": [
    "import bitstring\n",
    "\n",
    "f1 = bitstring.BitArray('float:32=-5.2123')\n",
    "print(f1.bin)\n",
    "\n",
    "f2 = bitstring.BitArray('float:64=5.4e31')\n",
    "print(f2.bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we used two floats: one is 32 bits (single precision) and the other is 64 bits (double precision). Note that the exact bits used to represent the floating point number is platform dependent. Most computers conform to the [IEEE 754 standard](https://en.wikipedia.org/wiki/IEEE_754). Within this standard: \n",
    "* a single precision float has 32 bits (1 bit for the sign, 8 bits for the exponent, and 23 for the value); thus it has 7 decimal digits of precision.\n",
    "* a double precision float has 64 bits (1 bit for the sign, 11 bits for the exponent, and 52 bits for the value); thus it has 16 decimal digits of precision.\n",
    "\n",
    "A double float has two times more precision then a single float. An example is shown below where the result of \"1.0/3.0\" is assigned to single and double precision float variables, respectively. Notice the difference in the number decimals and the difference (precision error) is about $10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1./3. (32 bits):  0.33333334\n",
      "1./3. (64 bits):  0.3333333333333333\n",
      "difference (precision errror):  9.934107481068821e-09\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x32=np.float32(1./3.)\n",
    "x64=np.float64(1./3.)\n",
    "diff = x32-x64\n",
    "\n",
    "print(\"1./3. (32 bits): \", x32)\n",
    "print(\"1./3. (64 bits): \", x64)\n",
    "print(\"difference (precision errror): \", diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we used the Numpy library. The functions \"float32\" and \"float64\" convert the number to single and doulbe precision, respectively. \n",
    "\n",
    "### Rounding error\n",
    "\n",
    "The root of rounding errors is the inexactness (error) in the representation of float point numbers on a digital computer and the arithmetic operations on them. To measure the roundoff error in the floating point number system, one can use the machine epsilon. Machine epsilon $\\epsilon_{machine}$ is defined as the smallest number on a computer such that\n",
    "\\begin{equation}\n",
    "1 + \\epsilon_{machine} > 1\n",
    "\\end{equation}\n",
    "It is the difference between 1 and the next nearest number representable on a computer. As such, a small trick to find out the machine precision is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine precision for double float:  2.220446049250313e-16\n",
      "machine precision for single float:  1.1920929e-07\n"
     ]
    }
   ],
   "source": [
    "print(\"machine precision for double float: \",abs(8./3 - 5./3 - 1.0))\n",
    "print(\"machine precision for single float: \",abs(np.float32(8./3) - np.float32(5./3) - np.float32(1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, the function \"abs\" is for the absolute value. By default, Python treat the float with double precision. Thus, the above code shows the machine precision for a double float and a single float is close to $2.2e^{-16}$ and $1.19e^{-9}$, respectively. Another way to find out the machine precision is to use Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.220446049250313e-16\n",
      "1.1920929e-07\n"
     ]
    }
   ],
   "source": [
    "print(np.finfo(float).eps)\n",
    "print(np.finfo(np.float32).eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the definition of machine epsilon, one can in fact calculate its value based on the following formula:\n",
    "\\begin{equation}\n",
    "\\epsilon_{machine} = b^{-(p-1)}\n",
    "\\end{equation}\n",
    "where $b$ is the base (2 for binary representation on digital computers), and $p$ is the number bits for the value (precision). Thus, \n",
    "\\begin{equation}\n",
    "\\epsilon_{machine} = b^{-(p-1)} = 2^{-(23-1)} \\approx 1.19e^{-9}\n",
    "\\end{equation}\n",
    "for single precision and \n",
    "\\begin{equation}\n",
    "\\epsilon_{machine} = b^{-(p-1)} = 2^{-(53-1)} \\approx 2.22e^{-16}\n",
    "\\end{equation}\n",
    "for double precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of values that single and double precision numbers is:\n",
    "* single precision: $\\pm 1.18 \\times 10^{-38} \\sim \\pm 3.4\\times 10^{38}$\n",
    "* double precision: $\\pm 2.23 \\times 10^{-308} \\sim \\pm 1.8\\times 10^{308}$\n",
    "\n",
    "Any number smaller than the lower bound of the range is called **underflow**, and any number larger than the upper bound of the range is called **overflow**. For example, the following code will result in an \"OverflowError\". Uncomment it to run and see the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_overflow = 2**(10000000000.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above background information on roundoff error, now we can show how it can affect arithmetic operations. When addition or subtraction is operated on two floating-point numbers, the computer will first shift one of the operands so that both operands have the same exponent, and then add or subtract the mantissas. The result is then rounded to the precision (decimal digits) utilized. Consequently, digits of precision in the smaller operand is lost. Let's use a 10-based number system and use only 4 decimal digits for precision. It is similar for the binary number system with single or double precision. \n",
    "\n",
    "Example: addition of values with different order of magnitude, e.g., 0.4321 and 0.008765. With 4 decimal digits, the numbers can be represented as $0.4321\\times 10^0$ and $0.8765 \\times 10^{-3}$. The addition process in computer is as follows:\n",
    "\\begin{equation}\n",
    "0.4321\\times 10^0 + 0.8765 \\times 10^{-3} = 0.4321\\times 10^0 + 0.0008765 \\times 10^{0} = 0.4329765 \\times 10^0\n",
    "\\end{equation}\n",
    "Then the result is rounded to $0.4339\\times 10^0$. The true solution should be $0.4329765\\times 10^0$. So the absolute error is $0.4339\\times 10^0 - 0.4329765\\times 10^0 = -0.0009235$.\n",
    "\n",
    "Example: subtraction of two close values. This example might be a little extreme. Let $a=1+0.5\\epsilon_{machine}$ and $b=1-0.5\\epsilon_{machine}$, then\n",
    "\\begin{equation}\n",
    "c = a-b = (1+0.5\\epsilon_{machine}) - (1-0.5\\epsilon_{machine}) = \\epsilon_{machine}\n",
    "\\end{equation}\n",
    "However, due to the cancellation of significant digits during the calculation, the following code shows that the result is only half of the true solution, i.e., 50% relative error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c            =  1.1102230246251565e-16\n",
      "true soltion =  2.220446049250313e-16\n"
     ]
    }
   ],
   "source": [
    "a = 1.0 + 0.5*np.finfo(float).eps\n",
    "b = 1.0 - 0.5*np.finfo(float).eps\n",
    "c = a - b\n",
    "print(\"c            = \", c)                    #print calculated c value\n",
    "print(\"true soltion = \",np.finfo(float).eps)   #for comparsion (true solution of c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error in the above example may seem not significant. They are very small anyway. However, sometime it becomes a big deal. One example to show the consequence of subtraction of two values of significant difference is the root of a quadratic function:\n",
    "\\begin{equation}\n",
    "f(x) = ax^2+bx+c = 0\n",
    "\\end{equation}\n",
    "which has a root in the form of \n",
    "\\begin{equation}\n",
    "x = \\frac{-b+\\sqrt{b^2-4ac}}{2a}\n",
    "\\end{equation}\n",
    "Here, the problem arises when $b$ is much larger than $a$ and $c$ such that  $\\sqrt{b^2-4ac} \\approx b$. Then, the numerator could be zero when subtracting the close values of $b$ and $\\sqrt{b^2-4ac}$ if not sufficient decimal digits is used in the computer to represent these numbers. For example, the following code use single precision for the calculation of one root of equation\n",
    "\\begin{equation}\n",
    "f(x) = x^2+10000x+c = 0\n",
    "\\end{equation}\n",
    "which gives the wrong value of 0.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a = np.float32(1.0)\n",
    "b = np.float32(10000.0)\n",
    "c = np.float32(1.0)\n",
    "\n",
    "root= (-b+np.sqrt(np.float32(b**2)-np.float32(4.0)*a*c))/(2.0*a)\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the default double precision for float, the result is reasonable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00010000000111176632\n"
     ]
    }
   ],
   "source": [
    "a = 1.0\n",
    "b = 10000.0\n",
    "c = 1.0\n",
    "\n",
    "root= (-b+np.sqrt(b**2-4.0*a*c))/(2.0*a)\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some techniques to reduce roundoff error:\n",
    "* use high precision: double, or even long double in Python.\n",
    "* reformulate the calculation. For example, the root of quadratic function can be reformulated as \n",
    "\\begin{equation}\n",
    "x = \\frac{-2c}{b+\\sqrt{b^2-4ac}}\n",
    "\\end{equation}\n",
    "where there is addition, instead of subtraction, in the denominator. \n",
    "* reduce the number of operations. One example is the evaluation of a $N$-th order polynomial in the form of\n",
    "\\begin{equation}\n",
    "P_N(x) = a_N + x ( a_{N-1} + x ( a_{N-2} + \\ldots + x (a_1 + a_0 x) \\ldots))\n",
    "\\end{equation}\n",
    "In this form, we only need to do $N$ additions and $N$ multiplications, in contrast to $N$ additions and ${N(N+1)}/{N}$ multiplications if we use the normal form of a polynomial. The effectiveness of approach is not universal. But it provides an option. The following is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1 =  -40.24921599999999\n",
      "res2 =  -40.249216\n"
     ]
    }
   ],
   "source": [
    "x=3.14\n",
    "res1=x**3-9.1*x**2+5.1*x+2.5\n",
    "res2=x*(x*(x-9.1)+5.1)+2.5\n",
    "\n",
    "print(\"res1 = \", res1)\n",
    "print(\"res2 = \", res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
